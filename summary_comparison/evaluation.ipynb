{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c87bfd6-e2f9-4fa3-ab20-962c83daec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import shutil\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce58748f-3c6a-4b7a-ba11-5b7d083bfcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed: ./submission_json/my_submission_bart_detail.json -> ./submissions/my_submission_bart_detail.json.gz\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output folder paths\n",
    "input_folder = \"./submission_json\"\n",
    "output_folder = \"./submissions\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    input_file_path = os.path.join(input_folder, file_name)\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_name}.gz\")\n",
    "\n",
    "    # Check if the current item is a file (not a folder)\n",
    "    if os.path.isfile(input_file_path):\n",
    "        # Open the input file and compress it into the output folder\n",
    "        with open(input_file_path, \"rb\") as f_in:\n",
    "            with gzip.open(output_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Compressed: {input_file_path} -> {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb1a55e7-8ef8-405d-8735-90aa9dae2f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Lilac_Fire\n",
      "https://en.wikipedia.org/wiki/Cranston_Fire\n",
      "https://en.wikipedia.org/wiki/Holy_Fire_(2018)\n",
      "https://en.wikipedia.org/wiki/Hurricane_Florence\n",
      "https://en.wikipedia.org/wiki/2018_Maryland_flood\n",
      "https://en.wikipedia.org/wiki/Saddleridge_Fire\n",
      "https://en.wikipedia.org/wiki/Hurricane_Laura\n",
      "https://en.wikipedia.org/wiki/Hurricane_Sally\n",
      "https://en.wikipedia.org/wiki/2020_Beirut_explosion\n",
      "https://en.wikipedia.org/wiki/2020_Houston_explosion\n",
      "https://en.wikipedia.org/wiki/Edenville_Dam#Dam_failure\n",
      "https://en.wikipedia.org/wiki/Hurricane_Dorian\n",
      "https://en.wikipedia.org/wiki/Kincade_Fire\n",
      "https://en.wikipedia.org/wiki/2020_Easter_tornado_outbreak\n",
      "https://en.wikipedia.org/wiki/Tornado_outbreak_of_April_22-23,_2020\n",
      "https://en.wikipedia.org/wiki/Tornado_outbreak_of_March_2-3,_2020\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import wikipedia\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "def get_wiki_summary(url):\n",
    "    \n",
    "    if \"wikipedia.org\" not in url:\n",
    "        return \"\"\n",
    "    \n",
    "    page_title = url.rpartition(\"/\")[-1]\n",
    "    print(url)\n",
    "    page = wikipedia.page(title=page_title, auto_suggest=False)\n",
    "    \n",
    "    return page.summary\n",
    "event_df = pd.read_json(\"CrisisFACTs-2022to2023.topics.json\", lines=False).set_index(\"eventID\")\n",
    "event_df[\"wiki.summary\"] = event_df[\"url\"].apply(get_wiki_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91af94ba-d2f8-44ac-acb3-f6f0b2d35079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfe3a87a-ebda-4620-bafe-639c945f02c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final-annotated-facts-results.json\", \"r\") as in_file:\n",
    "    annotation_data = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af3fb2fc-8f04-4338-b0a1-279dfd65f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions/my_submission_bart_detail.json.gz --> my_submission_bart_detail.json\n",
      "CrisisFACTS-001 42618\n",
      "CrisisFACTS-002 42618\n",
      "CrisisFACTS-003 42618\n",
      "CrisisFACTS-004 42618\n",
      "CrisisFACTS-005 42618\n",
      "CrisisFACTS-006 42618\n",
      "CrisisFACTS-007 42618\n",
      "CrisisFACTS-008 42618\n",
      "CrisisFACTS-009 42618\n",
      "CrisisFACTS-010 42618\n",
      "CrisisFACTS-011 42618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-012 42618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty reference sentence detected; setting raw BERTScores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-013 42618\n",
      "CrisisFACTS-014 42618\n",
      "CrisisFACTS-015 42618\n",
      "CrisisFACTS-016 42618\n",
      "CrisisFACTS-017 42618\n",
      "CrisisFACTS-018 42618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metric\n",
       "f1           0.628951\n",
       "precision    0.623133\n",
       "recall       0.635148\n",
       "Name: value, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_metrics = {}\n",
    "TOP_K = 32\n",
    "all_req_ids = list(annotation_data.keys())\n",
    "\n",
    "for f in glob.glob(\"submissions/*.gz\"):\n",
    "    \n",
    "    this_run_id = f.partition(\"/\")[-1].replace(\".gz\", \"\")\n",
    "    print(f, \"-->\", this_run_id)\n",
    "    \n",
    "    this_run_event_request_facts = {k:[] for k in all_req_ids}\n",
    "    \n",
    "    with gzip.open(f, \"r\") as in_file:\n",
    "        for line_ in in_file:\n",
    "            line = line_.decode(\"utf8\")\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            this_req_id = entry[\"requestID\"]\n",
    "            \n",
    "            # We skip days where we have no relevant facts from assessors\n",
    "            if this_req_id not in all_req_ids:\n",
    "                continue\n",
    "            \n",
    "            this_run_event_request_facts[this_req_id].append(entry)\n",
    "            \n",
    "    local_event_summaries = {e_id:[] for e_id in event_df.index}\n",
    "    for event_request,this_fact_list in this_run_event_request_facts.items():\n",
    "        event_id = event_request.rpartition(\"-\")[0]\n",
    "        \n",
    "        sorted_fact_list = sorted(this_fact_list, key=lambda v: v[\"importance\"], reverse=True)\n",
    "        this_day_summary = [this_top_fact[\"factText\"] for this_top_fact in sorted_fact_list[:TOP_K]]\n",
    "\n",
    "        \n",
    "        local_event_summaries[event_id] = local_event_summaries[event_id] + this_day_summary\n",
    "        \n",
    "    wiki_dfs = []\n",
    "\n",
    "    for event_id,event in event_df.iterrows():\n",
    "        \n",
    "        this_submitted_summary = local_event_summaries[event_id]\n",
    "\n",
    "        try:\n",
    "            this_summary_text = \". \".join(this_submitted_summary).replace(\"..\", \".\")\n",
    "        except:\n",
    "            pass\n",
    "        print(event_id, len(this_summary_text))\n",
    "        \n",
    "        wiki_summary = event[\"wiki.summary\"]\n",
    "        wiki_metric_ = bert_score.score([this_summary_text], [wiki_summary], model_type=\"roberta-large-mnli\")\n",
    "\n",
    "        wiki_metric = {\n",
    "            \"precision\": wiki_metric_[0],\n",
    "            \"recall\": wiki_metric_[1],\n",
    "            \"f1\": wiki_metric_[2],\n",
    "        }\n",
    "\n",
    "        this_wiki_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in wiki_metric.items()])\n",
    "\n",
    "        wiki_dfs.append(this_wiki_df)\n",
    "\n",
    "        \n",
    "    full_wiki_df = pd.concat(wiki_dfs)\n",
    "\n",
    "    submission_metrics[this_run_id] = {\n",
    "        \"wiki\": full_wiki_df\n",
    "    }\n",
    "    \n",
    "    display(full_wiki_df.groupby(\"metric\")['value'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c7899b-1f8d-498c-a95c-cd13bac66913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'my_submission_bart_detail.json': {'wiki':       metric     value            event\n",
       "  0  precision  0.720050  CrisisFACTS-001\n",
       "  1     recall  0.745512  CrisisFACTS-001\n",
       "  2         f1  0.732560  CrisisFACTS-001\n",
       "  0  precision  0.705777  CrisisFACTS-002\n",
       "  1     recall  0.743536  CrisisFACTS-002\n",
       "  2         f1  0.724165  CrisisFACTS-002\n",
       "  0  precision  0.704892  CrisisFACTS-003\n",
       "  1     recall  0.738438  CrisisFACTS-003\n",
       "  2         f1  0.721275  CrisisFACTS-003\n",
       "  0  precision  0.702079  CrisisFACTS-004\n",
       "  1     recall  0.679659  CrisisFACTS-004\n",
       "  2         f1  0.690687  CrisisFACTS-004\n",
       "  0  precision  0.696601  CrisisFACTS-005\n",
       "  1     recall  0.724576  CrisisFACTS-005\n",
       "  2         f1  0.710313  CrisisFACTS-005\n",
       "  0  precision  0.713770  CrisisFACTS-006\n",
       "  1     recall  0.741214  CrisisFACTS-006\n",
       "  2         f1  0.727233  CrisisFACTS-006\n",
       "  0  precision  0.703186  CrisisFACTS-007\n",
       "  1     recall  0.698702  CrisisFACTS-007\n",
       "  2         f1  0.700937  CrisisFACTS-007\n",
       "  0  precision  0.702774  CrisisFACTS-008\n",
       "  1     recall  0.677780  CrisisFACTS-008\n",
       "  2         f1  0.690051  CrisisFACTS-008\n",
       "  0  precision  0.710732  CrisisFACTS-009\n",
       "  1     recall  0.713458  CrisisFACTS-009\n",
       "  2         f1  0.712093  CrisisFACTS-009\n",
       "  0  precision  0.705546  CrisisFACTS-010\n",
       "  1     recall  0.728404  CrisisFACTS-010\n",
       "  2         f1  0.716792  CrisisFACTS-010\n",
       "  0  precision  0.000000  CrisisFACTS-011\n",
       "  1     recall  0.000000  CrisisFACTS-011\n",
       "  2         f1  0.000000  CrisisFACTS-011\n",
       "  0  precision  0.000000  CrisisFACTS-012\n",
       "  1     recall  0.000000  CrisisFACTS-012\n",
       "  2         f1  0.000000  CrisisFACTS-012\n",
       "  0  precision  0.686698  CrisisFACTS-013\n",
       "  1     recall  0.700855  CrisisFACTS-013\n",
       "  2         f1  0.693704  CrisisFACTS-013\n",
       "  0  precision  0.691351  CrisisFACTS-014\n",
       "  1     recall  0.678222  CrisisFACTS-014\n",
       "  2         f1  0.684724  CrisisFACTS-014\n",
       "  0  precision  0.701277  CrisisFACTS-015\n",
       "  1     recall  0.733029  CrisisFACTS-015\n",
       "  2         f1  0.716801  CrisisFACTS-015\n",
       "  0  precision  0.705383  CrisisFACTS-016\n",
       "  1     recall  0.701678  CrisisFACTS-016\n",
       "  2         f1  0.703526  CrisisFACTS-016\n",
       "  0  precision  0.677981  CrisisFACTS-017\n",
       "  1     recall  0.711088  CrisisFACTS-017\n",
       "  2         f1  0.694140  CrisisFACTS-017\n",
       "  0  precision  0.688290  CrisisFACTS-018\n",
       "  1     recall  0.716506  CrisisFACTS-018\n",
       "  2         f1  0.702115  CrisisFACTS-018}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d974fd-fd6e-4c19-be64-4e6a882b9db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
