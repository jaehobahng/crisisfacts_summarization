# âœ¨ Chrisis Summarization âœ¨

Our python package `crisis_summary` uses a rank --> rerank --> summarization pipeline to summarize what happened in a given day in a specific event to create visibility on important information and provide answers and sources.

## Features 

### Summary Package

#### Key Variables
- **`eventNo`**: 
- **`days`**: 
- **`model`**: 
- **`rerank`**: 
- **`summarize`**: 
- **`save`**: 

#### Algorithm Steps
1. **Read in Dataset / Queries**: Store in separate dictionary
2. **Ranking**: Break each document into overlapping word sequences (k-shingles) to capture local structure.
3. **Reranking**: Apply multiple hash functions to each documentâ€™s shingles, keeping only the minimum hash per function to create a signature matrix where columns are documents and rows are hash functions.
4. **Summarization**: Split each signature into band(groups of rows) â” Within each band, the sequence of hash values is grouped and treated as a single entity â” If two documents have the same band hash they are considered to be in the same "bucket" for that band.

## Requirements
**These will be the technical requirements to run the code**
- python = "^3.11"
- pandas = "^2.2.3"
- requests = "^2.32.3"
- ir-datasets = {git = "https://github.com/allenai/ir_datasets.git", rev = "crisisfacts"}
- pyterrier-t5 = {git = "https://github.com/terrierteam/pyterrier_t5.git"}
- rerankers = "^0.6.0"
- openai = "^1.55.1"
- sphinx = "^8.1.3"
- myst-nb = "^1.1.2"
- autoapi = "^2.0.1"
- sphinx-autoapi = "^3.3.3"
- sphinx-rtd-theme = "^3.0.2"
- pytest = "^8.3.3"
- python-dotenv = "^1.0.1"

## Installation

**We describe the bash comments to run each file**

```bash
pip install crisis_summary
```

## Usage

Explore [Usage Explanation Notebook](./example.ipynb) for a more detailed explanation of the package

### How to run package in script or notebook
#### Running BM25 --> ColBERT --> GPT Summary

```{python}
from crisis_summary.summary import crisis
from crisis_summary.utils.util import get_eventsMeta
from dotenv import load_dotenv
import os

load_dotenv('.env')
api = os.getenv("OPENAI_API_KEY")
os.environ['IR_DATASETS_HOME'] = './' # set to where crisisfacts repository is

eventsMeta = get_eventsMeta(eventNoList='001', days=2)

model = crisis(events = eventsMeta)

final_df, time_taken, memory_used = model.rank_rerank_colbert(model = 'BM25')
result_df = model.group_doc(final_df)
summary_df, time_summary, memory-summary = model.gpt_summary(result_df, api)
```


### How to run package from a terminal

Arguments:
- "-e", "--eventNo", - Required, "Event numbers to retrieve"
- "-d", "--days", Optional, "Number of days to retrieve"
- "-m", "--model", Optional, "Ranking model"
- "-r", "--rerank", Optional, "reranking model"
- "-u", "--summarize", Optional, "y/n for gpt summarization"
- "-s", "--save", Optional, "y/n to save output"

Example Terminal Code:
- python -m crisis_summary -e '001'
- python -m crisis_summary -e '001' -d 20 -m 'BM25' -r 'COLBERT'
- PYTHON -m crisis_summary -e '001' -d 20 -m 'BM25' -r 'COLBERT' -u 'y' -s 'y'

## Structure

Below is a brief overview of our file structure. We have also added a key to denote additional information about each file

ğŸš« - Part of .gitignore and not included in the repo 

ğŸ› ï¸ - Part of sphinx or package build, decription of file is not required 

ğŸ¨ - Images used in discussion.md, please see this file for further explanation

ğŸ“š - Please see sphinx documentation for a detailed description of this file

```
â”œâ”€â”€ ğŸ“ data (ğŸš«)
â”œâ”€â”€ ğŸ“ docs 
â”‚   â”œâ”€â”€ _build (ğŸ› ï¸)
â”‚   â”œâ”€â”€ Makefile (ğŸ› ï¸)
â”‚   â”œâ”€â”€ conf.py (ğŸ› ï¸)
â”‚   â”œâ”€â”€ example.ipynb â†’ demonstrates an example of usage of this package
â”‚   â”œâ”€â”€ index.md (ğŸ› ï¸)
â”‚   â”œâ”€â”€ make.bat (ğŸ› ï¸)
â”‚   â””â”€â”€ requirements.txt â†’ required dependencies for this package
â”œâ”€â”€ ğŸ“ images
â”‚   â”œâ”€â”€ LSH_Band_Row.png (ğŸ¨)
â”‚   â”œâ”€â”€ LSH_Shingle_Length.png (ğŸ¨)
â”‚   â”œâ”€â”€ LSH_number_trees.png (ğŸ¨)
â”‚   â”œâ”€â”€ Viz1.png (ğŸ¨)
â”‚   â”œâ”€â”€ Viz2.png (ğŸ¨)
â”‚   â”œâ”€â”€ bloom_filter_results.png (ğŸ¨)
â”‚   â”œâ”€â”€ hash_functions_FP_rate.png (ğŸ¨)
â”‚   â”œâ”€â”€ lsh_improved_params.png (ğŸ¨)
â”‚   â””â”€â”€ lsh_s_curve.png (ğŸ¨)
â”œâ”€â”€ ğŸ“ notebooks
â”‚   â”œâ”€â”€ EDA.ipynb â†’ this notebook shows the EDA conducted on our text data
â”‚   â”œâ”€â”€ exercise.ipynb â†’ solves BloomFilter textbook problems
â”‚   â”œâ”€â”€ lsh.ipynb â†’ Used to test our implementation of LSH prior to the final version
â”‚   â”œâ”€â”€ test.ipynb â†’ Used to test our implementation of multi probe LSH prior to the final version
â”‚   â”œâ”€â”€ visualization_lsh.ipynb â†’ code for LSH graphs 
â”‚   â””â”€â”€ visualizations.ipynb â†’ code for LSH graphs 
â”œâ”€â”€ ğŸ“ output â†’ the files within this directory are the desired output for each data file for each near-deduplication algorithm
â”‚   â”œâ”€â”€ hundredk-LSH_mp.txt
â”‚   â”œâ”€â”€ hundredk-baseline.txt
â”‚   â”œâ”€â”€ hundredk-lsh.txt
â”‚   â”œâ”€â”€ onek-LSH.txt
â”‚   â”œâ”€â”€ onek-LSH_mp.txt
â”‚   â”œâ”€â”€ onek-baseline.txt
â”‚   â”œâ”€â”€ tenk-LSH_mp.txt
â”‚   â”œâ”€â”€ tenk-baseline.txt
â”‚   â”œâ”€â”€ tenk-lsh.txt
â”‚   â”œâ”€â”€ threehundred-LSH_mp.txt
â”‚   â”œâ”€â”€ threehundred-baseline.txt
â”‚   â””â”€â”€ threehundred-lsh.txt
â”œâ”€â”€ ğŸ“ src
â”‚   â”œâ”€â”€ ğŸ“ deduplication
â”‚   â”‚   â”œâ”€â”€ LSH.py (ğŸ“š)
â”‚   â”‚   â”œâ”€â”€ LSHForest.py (ğŸ“š)
â”‚   â”‚   â”œâ”€â”€ LSHImproved.py (ğŸ“š)
â”‚   â”‚   â”œâ”€â”€ __init__.py (ğŸ“š)
â”‚   â”‚   â”œâ”€â”€ __main__.py (ğŸ“š)
â”‚   â”‚   â”œâ”€â”€ bloom_filter.py (ğŸ“š)
â”‚   â”‚   â”œâ”€â”€ dedup.py (ğŸ“š)
â”‚   â””â”€â”€ ğŸ“ utils
â”‚       â”œâ”€â”€ use_cases.py (ğŸ“š)
â”‚       â”œâ”€â”€ utils.py (ğŸ“š)
â”‚       â”œâ”€â”€ visualization_lsh.py (ğŸ“š)
â”‚       â””â”€â”€ visualizations.py (ğŸ“š)
â””â”€â”€ ğŸ“ tests
â”‚   â”œâ”€â”€ test_deduplication.py â†’ test cases for each near-deduplication algorithm
â”œâ”€â”€ discussion.md â†’ describes algorithms in greater detail and answers project questions
â”œâ”€â”€ poetry.lock (ğŸ› ï¸)
â”œâ”€â”€ pyproject.toml (ğŸ› ï¸)
```


## Contributing

Clone and set up the repository with

```bash
git clone TODO && cd deduplication
pip install -e ".[dev]"
```

Install pre-commit hooks with

```bash
pre-commit install
```

Run tests using

```
pytest -v tests
```