{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c451d1b-d39c-4f1d-bcab-bc444c244e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import psutil\n",
    "import shutil\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7357ebb2-60e0-471c-9700-03d99d45fa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./result_df_before_summary.csv\")\n",
    "df['date'] = pd.to_datetime(df['unix_timestamp'], unit='s').dt.date\n",
    "df1 = df[df['request_id'].str.startswith(\"CrisisFACTS-001-\")]\n",
    "\n",
    "bart_summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-xsum\")\n",
    "pega_summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31cc8292-ea06-4219-b845-b963a85e1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "# Set up parallel processing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "num_batches = num_cores * 2\n",
    "\n",
    "# Split the dataset into batches\n",
    "batches = np.array_split(df1['texts'], num_batches)\n",
    "\n",
    "# Function to clean and summarize text\n",
    "def clean_and_summarize(text, summarizer):\n",
    "    # Remove hashtags, URLs, and mentions\n",
    "    cleaned_text = re.sub(r'#\\S+|https?://\\S+|@\\S+', '', text)\n",
    "    # Generate summary\n",
    "    summary = summarizer(cleaned_text, max_length=20, min_length=10)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Wrapper function for batch processing\n",
    "def process_batch(texts, summarizer):\n",
    "    return [clean_and_summarize(text, summarizer) for text in texts]\n",
    "\n",
    "\n",
    "def summarize(summarizer):\n",
    "    # Apply parallel processing\n",
    "    results = Parallel(n_jobs=num_cores)(\n",
    "        delayed(process_batch)(batch.tolist(), summarizer) for batch in batches\n",
    "    )\n",
    "\n",
    "    summaries = [summary for batch_result in results for summary in batch_result]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81c1e65b-5c8a-4077-bb3f-4cd6d7e05d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187.6734289481294\n",
      "36.46804\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "api = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "openai.api_key = api\n",
    "\n",
    "process = psutil.Process(os.getpid())  # Get current process\n",
    "start_memory = process.memory_info().rss  # Memory usage at start (in bytes)\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "answer_output = []\n",
    "for i, row in df.iterrows():\n",
    "    question = str(row['question'] + \"?\")\n",
    "    provided_text = row['texts']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Answer the question based only on the text provided below. \n",
    "    If no answers can be found at all, return \"unanswerable\"\n",
    "\n",
    "    Don't make the responses conversational.\n",
    "    Expressions like hundreds of thousands can be answers to questions asking how many or how much.\n",
    "    Do not line break the text and just give me the output.\n",
    "\n",
    "    Text:\n",
    "    {provided_text}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "\n",
    "    client = openai.OpenAI()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    answer_output.append(answer)\n",
    "    # Print progress every 10 loops\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Processed {i + 1} rows\")\n",
    "\n",
    "# Calculate runtime and memory usage\n",
    "end_time = time.time()  # End time\n",
    "end_memory = process.memory_info().rss  # Memory usage at end (in bytes)\n",
    "runtime = end_time - start_time\n",
    "memory_used = (end_memory - start_memory) / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "print(runtime)\n",
    "print(memory_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a03817fa-5886-4674-b404-01556f3691b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "/opt/conda/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 20, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Your max_length is set to 20, but your input_length is only 16. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Your max_length is set to 20, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "Your max_length is set to 20, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613.3333511352539\n",
      "340.77734375\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())  # Get current process\n",
    "start_memory = process.memory_info().rss  # Memory usage at start (in bytes)\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "pega_summary = summarize(pega_summarizer)\n",
    "\n",
    "# Calculate runtime and memory usage\n",
    "end_time = time.time()  # End time\n",
    "end_memory = process.memory_info().rss  # Memory usage at end (in bytes)\n",
    "runtime = end_time - start_time\n",
    "memory_used = (end_memory - start_memory) / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "print(runtime)\n",
    "print(memory_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4afe209-c1c9-4e71-a594-eccdebaac606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 20, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n",
      "Your max_length is set to 20, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427.0489273071289\n",
      "2.078125\n"
     ]
    }
   ],
   "source": [
    "process = psutil.Process(os.getpid())  # Get current process\n",
    "start_memory = process.memory_info().rss  # Memory usage at start (in bytes)\n",
    "start_time = time.time()  # Start time\n",
    "\n",
    "bart_summary = summarize(bart_summarizer)\n",
    "\n",
    "# Calculate runtime and memory usage\n",
    "end_time = time.time()  # End time\n",
    "end_memory = process.memory_info().rss  # Memory usage at end (in bytes)\n",
    "runtime = end_time - start_time\n",
    "memory_used = (end_memory - start_memory) / 1024 / 1024  # Convert bytes to MB\n",
    "\n",
    "print(runtime)\n",
    "print(memory_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f3d101e-0a00-4c37-a60a-d273db999263",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = df.copy()\n",
    "df_pega = df.copy()\n",
    "df_bart = df.copy()\n",
    "\n",
    "df_gpt[\"summary\"] = None\n",
    "df_pega[\"summary\"] = None\n",
    "df_bart[\"summary\"] = None\n",
    "\n",
    "df_gpt.iloc[0:434, df_gpt.columns.get_loc(\"summary\")] = answer_output\n",
    "df_pega.iloc[0:434, df_pega.columns.get_loc(\"summary\")] = pega_summary\n",
    "df_bart.iloc[0:434, df_bart.columns.get_loc(\"summary\")] = bart_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5443edef-4f5a-463a-91ac-e0f282cc308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./submission_json/my_submission_pega_detail.json\", 'w') as f:\n",
    "    for index, row in df_pega.iterrows():\n",
    "        fact_ = {\n",
    "            \"requestID\": row['request_id'],\n",
    "            \"factText\": row['summary'],\n",
    "            \"unixTimestamp\": int(row['unix_timestamp']),\n",
    "            \"importance\": float(row['avg_importance']),\n",
    "            \"sources\": row['docno_list'],\n",
    "            \"streamID\": None,\n",
    "            \"informationNeeds\": row['q_id']\n",
    "        }\n",
    "        \n",
    "        # Write each dictionary as a separate JSON object on a new line\n",
    "        json.dump(fact_, f)\n",
    "        f.write(\"\\n\")  # Add a newline after each JSON object\n",
    "\n",
    "with open(\"./submission_json/my_submission_bart_detail.json\", 'w') as f:\n",
    "    for index, row in df_bart.iterrows():\n",
    "        fact_ = {\n",
    "            \"requestID\": row['request_id'],\n",
    "            \"factText\": row['summary'],\n",
    "            \"unixTimestamp\": int(row['unix_timestamp']),\n",
    "            \"importance\": float(row['avg_importance']),\n",
    "            \"sources\": row['docno_list'],\n",
    "            \"streamID\": None,\n",
    "            \"informationNeeds\": row['q_id']\n",
    "        }\n",
    "        \n",
    "        # Write each dictionary as a separate JSON object on a new line\n",
    "        json.dump(fact_, f)\n",
    "        f.write(\"\\n\")  # Add a newline after each JSON object\n",
    "\n",
    "with open(\"./submission_json/my_submission_gpt_detail.json\", 'w') as f:\n",
    "    for index, row in df_gpt.iterrows():\n",
    "        fact_ = {\n",
    "            \"requestID\": row['request_id'],\n",
    "            \"factText\": row['summary'],\n",
    "            \"unixTimestamp\": int(row['unix_timestamp']),\n",
    "            \"importance\": float(row['avg_importance']),\n",
    "            \"sources\": row['docno_list'],\n",
    "            \"streamID\": None,\n",
    "            \"informationNeeds\": row['q_id']\n",
    "        }\n",
    "        \n",
    "        # Write each dictionary as a separate JSON object on a new line\n",
    "        json.dump(fact_, f)\n",
    "        f.write(\"\\n\")  # Add a newline after each JSON object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14a00a96-8e3f-4b03-a60c-c966a4b983c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed: ./submission_json/my_submission_gpt_detail.json -> ./submissions/my_submission_gpt_detail.json.gz\n",
      "Compressed: ./submission_json/my_submission_bart_detail.json -> ./submissions/my_submission_bart_detail.json.gz\n",
      "Compressed: ./submission_json/my_submission_pega_detail.json -> ./submissions/my_submission_pega_detail.json.gz\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"./submission_json\"\n",
    "output_folder = \"./submissions\"\n",
    "\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Loop through all files in the input folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    input_file_path = os.path.join(input_folder, file_name)\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_name}.gz\")\n",
    "\n",
    "    # Check if the current item is a file (not a folder)\n",
    "    if os.path.isfile(input_file_path):\n",
    "        # Open the input file and compress it into the output folder\n",
    "        with open(input_file_path, \"rb\") as f_in:\n",
    "            with gzip.open(output_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Compressed: {input_file_path} -> {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b8327-719b-4f4d-85bb-9b7fac2a8487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
