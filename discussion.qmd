---
title: "Discussion"
date: "`r Sys.Date()`"
format: pdf
---

# Discussion

**1. EDA of Dataset**

The **CrisisFACTS Dataset** supports research on information needs during crises through two key components: the Event Dataset and the Query Dataset. The Event Dataset contains over 1.2 million crisis-related documents with metadata like event names, timestamps, and source types, enabling analysis of information flow during events.

The Query Dataset includes 922 user-generated queries linked to events, categorized by intent (e.g., fact-finding or assistance requests) with metadata like indicative terms and descriptions. Together, these datasets provide a foundation for studying information retrieval and summarization in crisis scenarios. The following EDA explores their structure and insights.

- Event Dataset overview
   ```
   <class 'pandas.core.frame.DataFrame'>
   RangeIndex: 1220703 entries, 0 to 1220702
   Data columns (total 9 columns):
   ```

   | **#** | **Column**       | **Non-Null Count** | **Dtype**  |
   |-------|------------------|--------------------|------------|
   | 0     | event            | 1220703            | object     |
   | 1     | text             | 1220703            | object     |
   | 2     | source           | 1220703            | object     |
   | 3     | source_type      | 1220703            | object     |
   | 4     | unix_timestamp   | 1220703            | float64    |
   | 5     | docno            | 1220703            | object     |
   | 6     | ID               | 1220703            | int64      |
   | 7     | date             | 1220703            | object     |
   | 8     | EventName        | 1220703            | object     |

   The **Event Dataset** contains a total of 1,220,703 entries with 9 columns. Each entry corresponds to a document associated with a specific event, identified by the `event` and `EventName` columns. The dataset provides detailed information such as the text content (`text`), its source (`source` and `source_type`), a unique document identifier (`docno`), and the timestamp (`unix_timestamp`) when the document was generated. The dataset is comprehensive and complete, with no missing values in any column.
   
   **Event Text Content Overview**

| **Event**          | **Text**                                                                                                                                                                             | **Source Type** |
|---------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| 001     | Live updates: San Diego County fire is 92 percent contained                                                                                                                         | News            |
| 001     | The Lilac fire now 92 percent contained, Cal Fire officials said Tuesday morning.                                                                                                   | News            |
| 001     | The county of San Diego has opened a Local Assistance Center to help victims of the fire begin the rebuilding and recovery process.                                                 | News            |
| 001     | The center is at the Vista branch library on 700 Eucalyptus Avenue and will be open from 9:00 a.m. to 6:00 p.m. Services offered will include crisis counseling and referral services.| News            |
| 001     | Homeowners also will be able to get information on residential rebuilding and consumer fraud.                                                                                       | News            |

   The `text` column in the event dataset contains detailed descriptions of updates, reports, and announcements related to crisis events. The entries provide various information about the event, such as live updates, recovery processes, or available services. These texts are extracted from sources like Reddit, Twitter, Facebook and News, and it will be used later in this project for information retrieval and summarization.

- Query Dataset overview
   ```
   <class 'pandas.core.frame.DataFrame'>
   RangeIndex: 922 entries, 0 to 921
   Data columns (total 13 columns):
   ```


   | **#** | **Column**                 | **Non-Null Count** | **Dtype**  |
   |-------|----------------------------|--------------------|------------|
   | 0     | query_id                  | 922                | object     |
   | 1     | text                      | 922                | object     |
   | 2     | indicative_terms          | 922                | object     |
   | 3     | trecis_category_mapping   | 814                | object     |
   | 4     | event_id                  | 922                | object     |
   | 5     | event_title               | 922                | object     |
   | 6     | event_dataset             | 409                | object     |
   | 7     | event_description         | 922                | object     |
   | 8     | event_trecis_id           | 922                | object     |
   | 9     | event_type                | 922                | object     |
   | 10    | event_url                 | 922                | object     |
   | 11    | ID                        | 922                | int64      |
   | 12    | EventName                 | 922                | object     |

   The **Query Dataset** consists of 922 entries across 13 columns, where each row represents a query linked to an event. Key columns include `query_id`, `text` (the query itself), and `indicative_terms` (keywords representing the query's focus). While most columns are fully populated, the `trecis_category_mapping` column has some missing values, indicating incomplete categorization for certain queries. Additionally, only 409 queries include information about the dataset they reference (`event_dataset`).

   **Query Text Content Overview**

| **query_id**             | **text**                              | **event_id**        |
|---------------------------|----------------------------------------|---------------------|
| CrisisFACTS-General-q001  | Have airports closed                  | CrisisFACTS-001     |
| CrisisFACTS-General-q002  | Have railways closed                  | CrisisFACTS-001     |
| CrisisFACTS-Wildfire-q001 | What area has the wildfire burned     | CrisisFACTS-003     |
| CrisisFACTS-Hurricane-q001| What is the hurricane category        | CrisisFACTS-004     |
| CrisisFACTS-Tornado-q001  | What storm warnings are active        | CrisisFACTS-017     |

   The query text in the dataset represents user-generated questions or information needs during crisis events. These queries are designed to retrieve relevant information about specific aspects of the crises, such as their impact, status, or recovery efforts. They are categorized into **general queries**, which address broader topics like transportation disruptions or evacuation updates, and **specific queries**, which focus on particular crisis types, such as wildfires, hurricanes, accident, flood and tornadoes. The query text will be used later in the project pipeline to help with our information retrieval.
   
- Days, Queries and Documents Count for Each Event

   |  **Event**          | **Event Name**                    | **Unique Days** | **Query Count** | **Document Count** |
   |---------------------|-----------------------------------|-----------------|-----------------|--------------------|
   | CrisisFACTS-001     | Lilac Wildfire 2017              | 9               | 52              | 51,015             |
   | CrisisFACTS-002     | Cranston Wildfire 2018           | 6               | 52              | 30,535             |
   | CrisisFACTS-003     | Holy Wildfire 2018               | 7               | 52              | 32,489             |
   | CrisisFACTS-004     | Hurricane Florence 2018          | 14              | 51              | 120,784            |
   | CrisisFACTS-005     | 2018 Maryland Flood              | 3               | 48              | 13,000             |
   | CrisisFACTS-006     | Saddleridge Wildfire 2019        | 4               | 52              | 32,362             |
   | CrisisFACTS-007     | Hurricane Laura 2020             | 2               | 51              | 18,161             |
   | CrisisFACTS-008     | Hurricane Sally 2020             | 8               | 51              | 72,798             |
   | CrisisFACTS-009     | Beirut Explosion 2020            | 7               | 56              | 186,900            |
   | CrisisFACTS-010     | Houston Explosion 2020           | 7               | 56              | 67,956             |
   | CrisisFACTS-011     | Rutherford TN Floods 2020        | 5               | 48              | 20,868             |
   | CrisisFACTS-012     | TN Derecho 2020                  | 7               | 49              | 52,546             |
   | CrisisFACTS-013     | Edenville Dam Fail 2020          | 7               | 48              | 25,547             |
   | CrisisFACTS-014     | Hurricane Dorian 2019            | 7               | 51              | 116,133            |
   | CrisisFACTS-015     | Kincade Wildfire 2019            | 7               | 52              | 107,305            |
   | CrisisFACTS-016     | Easter Tornado Outbreak 2020     | 5               | 51              | 60,885             |
   | CrisisFACTS-017     | Tornado Outbreak April 2020      | 6               | 51              | 120,393            |
   | CrisisFACTS-018     | Tornado Outbreak March 2020      | 6               | 51              | 91,026             |

   The merged chart highlights the variability in the number of unique days, queries, and documents across different crisis events in the CrisisFACTS dataset. Events like **Hurricane Florence 2018** and the **Beirut Explosion 2020** stand out with a high number of documents (120,784 and 186,900, respectively), reflecting their significant information impact and media coverage. Conversely, smaller-scale events such as **Hurricane Laura 2020** and the **2018 Maryland Flood** have fewer documents (18,161 and 13,000, respectively). Most events have a consistent number of queries (around 51–56), indicating a balanced level of information need across crises, while the number of unique days varies significantly, with some events spanning only 2 days (e.g., **Hurricane Laura 2020**) and others lasting up to 14 days (e.g., **Hurricane Florence 2018**). 


**2. Ranking / Reranking Process**

 - **Ranking model comparison**
   Using PyTerrier's pre-stored initial indexes, we conducted a detailed analysis to evaluate and compare various ranking models, focusing on both efficiency and accuracy.

   For efficiency, we measured the memory consumption and processing time required for each model to handle a single data unit (representing one day of a specific event). Accuracy was assessed by using the rankings generated by re-ranking models as the benchmark, under the assumption that these models provide the most precise results. We calculated the Mean Squared Error (MSE) to determine how closely each model's rankings aligned with the re-ranked benchmarks.

   The results, summarized in the accompanying table, reveal that BM25 emerged as the top performer. It had the lowest MSE, indicating the highest accuracy, and also proved to be the fastest model. While its memory usage was slightly above average, its combination of speed and precision made it the most suitable choice for our experiment, leading us to adopt BM25 as the default model.


   | model         | memory    | time      | MSE        |
   |---------------|-----------|-----------|------------|
   | BM25          | 50.781250 | 6.514025  | 216.637500 |
   | DFRee         | 32.414062 | 11.001296 | 389.604167 |
   | DPH           | 38.562500 | 9.705055  | 404.720833 |
   | DirichletLM   | 85.347656 | 10.221884 | 297.254167 |
   | Hiemstra_LM   | 37.957031 | 9.004418  | 199.220833 |
   | InL2          | 89.050781 | 9.054682  | 222.350000 |
   | PL2           | 42.320312 | 6.021144  | 218.820833 |

   - Example text from BM25
      | text                                                                                             | rank | rerank_rank | model |  SE  | 
      |--------------------------------------------------------------------------------------------------|------|-------------|-------|------|
      | Full containment is not expected until early January.                                            | 1    | 1           | BM25  |  0   | 
      | #lilacfire is now 175 acres 0 containment #SanDiego County                                       | 14   | 2           | BM25  |  144 | 
      | Cal Fire reaches 100 percent containment on Lilac Fire                                           | 13   | 3           | BM25  |  100 | 
      | JUST IN: 500 acres, 0% Containment. #Breaking #LilacFire @fox5sandiego https://t.co/rHDRSaUwIC   | 37   | 4           | BM25  | 1089 | 
      | As of Dec 6th, @ 1pm- #ThomasFire has burned over 65,000 acres and has 0% containment.           | 31   | 5           | BM25  |  676 | 

   - Example text from DFRee
      | text                                                                                                              | rank | rerank_rank | model | SE   |
      |-------------------------------------------------------------------------------------------------------------------|------|-------------|-------|------|
      | JUST IN: 500 acres, 0% Containment. #Breaking #LilacFire @fox5sandiego https://t.co/rHDRSaUwIC                    | 10   | 1           | DFRee | 81   |
      | As of Dec 6th, @ 1pm- #ThomasFire has burned over 65,000 acres and has 0% containment.                            | 4    | 2           | DFRee | 4    |
      | Fires have now hit San Diego County ... lets hope for a quick containment https://t.co/CqMGZhJxXo                 | 38   | 3           | DFRee | 1225 |
      | #LilacFire Update: 100-150 Acres and 0% Containment https://t.co/kjdaaahl6l                                       | 25   | 4           | DFRee | 441  |
      | So far the fire has not reached Oceanside or Vista. Praying they get containment soon! https://t.co/I3JyzDmGDY    | 11   | 5           | DFRee | 36   |


 - **Reranking model comparison**
   Since there was no ground truth available for measuring accuracy with reranking, we relied on research papers for model comparisons and focused on those discussed in lectures. We tested the ColBERT model and a mono-T5 model, both available in the PyTerrier package. Referring to the [documentation](https://www.answer.ai/posts/2024-08-13-small-but-mighty-colbert.html) on Huggingface, we selected a mini ColBERT model as our primary re-ranking approach, balancing efficiency while benchmarking it against the mono-T5 model for comparative analysis.


   | model   | memory       | time      |
   |---------|--------------|-----------|
   | COLBERT | 798.367188   | 13.036193 |
   | T5      | 1298.781250  | 32.529773 |

   
   - Colbert
      | text                                                                                                                                                                                                   | rerank | importance | query                      |
      |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|------------|----------------------------|
      | [[462655153,C]] [[462655153,C]] Evacuation Shelters An overnight shelter was set up for residents at the East Valley Community Center on 2245 E. Valley Parkway in Escondido.                          | 1      | 0.382477   | evacuation center shelters |
      | The Red Cross-operated full-service evacuation shelter is located at East Valley Community Center (2245 E. Valley P https://t.co/j04N921sd                                                             | 2      | 0.381914   | evacuation center shelters |
      | @CALFIRESANDIEGO: #LilacFire [update] 3 Temporary evacuation shelters have been set up: Fallbrook High School Pechanga Casino Pala Casino East Valley Community Center in Escondido is where the ev... | 3      | 0.380616   | evacuation center shelters |
      | EVACUATION SHELTERS EL CAJON: Bostonia Park & Recreation Center (1049 Bostonia Street)* SAN MARCOS: Palomar College (1140 West Mission Road)*Small pet-friendly As of Saturday, the only she...        | 4      | 0.376959   | evacuation center shelters |
      | ***Temporary Evacuation Shelters*** #LilacFire https://t.co/j43zUXR4ze                                                                                                                                 | 5      | 0.373286   | evacuation center shelters |

   - T5
      | text                                                                                                                                                                                                   | rerank | importance | query                      |
      |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------|------------|----------------------------|
      | EVACUATION SHELTERS EL CAJON: Bostonia Park & Recreation Center (1049 Bostonia Street)* SAN MARCOS: Palomar College (1140 West Mission Road)*Small pet-friendly As of Saturday, the only she...        | 1      | 1.000000   | evacuation center shelters |
      | [[462655153,C]] [[462655153,C]] Evacuation Shelters An overnight shelter was set up for residents at the East Valley Community Center on 2245 E. Valley Parkway in Escondido.                          | 2      | 0.999964   | evacuation center shelters |
      | The Red Cross-operated full-service evacuation shelter is located at East Valley Community Center (2245 E. Valley P... https://t.co/j04N921sd                                                          | 3      | 0.999836   | evacuation center shelters |
      | @CALFIRESANDIEGO: #LilacFire [update] 3 Temporary evacuation shelters have been set up: Fallbrook High School Pechanga Casino Pala Casino East Valley Community Center in Escondido is where the ev... | 4      | 0.999406   | evacuation center shelters |
      | Mandatory evacuations were in place for surrounding areas, while evacuation shelters were set up at Fallbrook High School, Pala Casino and East Valley Community Center.                               | 5      | 0.999263   | evacuation center shelters |


**3. Summarization Process**
Having Ranked-Reranked dataset, we experimented with three pre-trained LLMs for the summarization process: Facebook BART, Google Pegasus, and OpenAI GPT. The dataset is groupby request ID, query ID, and event attributes to get a grouped document text. This grouped document text will be passed into the summarization model to get a one single text regarding that specific request ID, date, and query. To choose which model to use for the package, we focused on one event for comparison.

Facebook BART and Google Pegasus are implemented through Hugging Face Transformers. Grouped document texts are cleaned, removing hashtags, URLs, and mentions. OpenAI GPT, however, adopts a slightly different approach. Since we have a `question` column, we asked the model to answer that question using the grouped document text. If the question is answerable, we asked the model to produce the most concise answer as possible. If not, it returns "unanswerable".

 - Summarization Process Example

|    | question                              | query                    | bart_summary                                                                            | pega_summary                                                                             | gpt_summary                                                                                                                                                                                   |
|---:|:--------------------------------------|:-------------------------|:----------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 20 | What roads are blocked / closed       | tree block road closures | Motorists are being warned to expect delays after a number of crashes on the I-805      | Accident, three lanes blocked in on 8 EB at Severin Dr, stopped traffic back to          | The two right lanes on 8 EB at Severin Dr, the three right lanes on 8 EB at Severin Dr, the two right lanes on I-805 NB after Murray Rdg Rd, and the left lane on 15 SB before 5 are blocked. |
|  2 | Have water supplied been contaminated | water supply             | Firefighters in California have been using water from a nearby lake to fight wildfires. | A UH-1Y Venom refills its bucket firefighting system with water from Lake ONeill at Camp | unanswerable                                                                                                                                                                                  |

**4. Evaluation**

To evaluate and compare the performance of these summarization models, we followed the methodology outlined in the referenced paper. Specifically, we utilized a Wikipedia summary as a ground truth reference and assessed the model outputs using both BERTScore and ROUGE metrics. Each metric provided precision, recall, and F1 scores, enabling a comprehensive evaluation of the models' performance. As shown in tables below, OpenAI GPT model performed the best for our project due to its speed, minimal memory usage, and ability to produce concise summaries. In terms of evaluation, there was not much of difference across the model. BERTScore uses the contextual embeddings so we got higher scores because the query/questions are related to the grouped document texts. Yet ROUGE measures overlaps using n-grams which will decrease the score since the models may not use the same words as the original text. Ultimately, we decided to use OpenAI GPT for our project.

 - Time and Memory Usage Table

| Model         | memory     | time      |
|---------------|------------|-----------|
| Facebook BART | 1362.03    | 479s      | 
| Google Pegasus| 59.53      | 615s      | 
| OpenAI GPT    | 36.47      | 187s      | 

 - Evaluation Table

| Model         | Metric     | Precision | Recall | F1 Score |
|---------------|------------|-----------|--------|----------|
| Facebook BART | BERTScore  | 0.72      | 0.75   | 0.73     |
| Google Pegasus| BERTScore  | 0.72      | 0.74   | 0.73     |
| OpenAI GPT    | BERTScore  | 0.70      | 0.73   | 0.71     |
| Facebook BART | ROUGE      | 0.006     | 0.32   | 0.01     |
| Google Pegasus| ROUGE      | 0.004     | 0.29   | 0.01     |
| OpenAI GPT    | ROUGE      | 0.01      | 0.15   | 0.03     |

**5. Challenges / Further improvements**
Our pipeline, consisting of ranking, re-ranking, and summarization steps, presented challenges when experimenting with every possible combination and hyperparameter to optimize performance. Adjustments ranged from straightforward changes, such as swapping models at each step, to more intricate refinements, including tweaking hyperparameter values to decimal precision or performing prompt engineering for the LLM-powered summarization model. Given the current output often includes summaries labeling questions as "unanswerable," it is crucial to ensure the model's robustness. This would help build user confidence in the system's responses, particularly when it determines that a query truly cannot be answered.







<br>

**Requirements**
- Include all of your analysis in a PDF in your repository called discussion.pdf. The discussion.pdf should be no longer than 4 pages, excluding references, and it will include your comments, graphics, and overall analysis. Your general structure might be something like:
    - exploratory data analysis over your dataset (e.g., collection size, collection characteristics, …)
    - details of engineering decisions such as preprocessing (e.g., discussing how you preprocess documents, did you use techniques like deduplication or other forms of preprocessing, …)
    - details of your search algorithms, possibly including improvements (e.g., you don’t need to elaborate in detail on the models you use, but you should make sure you provide baselines and talk about improvements you’ve made, just as we did in Assignment 2)
    - visualizations, results, and overall analysis (e.g., the survey papers provide examples of ways to visualize results and in class we learned about evaluating retrieval systems. If you choose to focus on the engineering of the problem (bigger dataset without an evaluation), then you will discuss how that looked for your team).
    - challenges you encountered and conclusions